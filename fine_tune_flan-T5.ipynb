{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a315dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c893ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba4c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff splits: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches.\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b78d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb1db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_datasets = tokenize_datasets.filter(lambda exmaple, index: index % 100 == 0,  with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bae065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(r=32, #rank 32,\n",
    "                         lora_alpha=32, ## LoRA Scaling factor \n",
    "                         target_modules=['q', 'v'], ## The modules(for example, attention blocks) to apply the LoRA update matrices.\n",
    "                         lora_dropout = 0.05,\n",
    "                         bias='none',\n",
    "                         task_type=TaskType.SEQ_2_SEQ_LM ## flan-t5\n",
    ")\n",
    "\n",
    "## target_modules='q', This represents the value projection layer in the transformer model. The value projection layer transforms input tokens into value vectors,\n",
    "# which are the actual values that are attended to based on the attention scores computed from query and key vectors.\n",
    "\n",
    "## target_modules='v',This typically refers to the query projection layer in a transformer-based model. The query projection layer is responsible for transforming \n",
    "# input tokens into query vectors, which are used to attend to other tokens in the sequence during self-attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f58b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb43dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                      './peft-dialogue-summary-checkpoint-local',\n",
    "                                      torch_dtype=torch.bfloat16,\n",
    "                                      is_trainable=False) ## is_trainable mean just a forward pass jsut to get a sumamry\n",
    "\n",
    "index = 200 ## randomly pick index\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f'Human Baseline summary: \\n{human_baseline_summary}\\n')\n",
    "print(f'Original Model Output \\n{original_model_text_output}\\n')\n",
    "print(f'Peft Model Output \\n{peft_model_text_output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94d53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -alh ./dialogue-summary-checkpoint-local/adapter_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44097c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                      './peft-dialogue-summary-checkpoint-local',\n",
    "                                      torch_dtype=torch.bfloat16,\n",
    "                                      is_trainable=False) ## is_trainable mean just a forward pass jsut to get a sumamry\n",
    "\n",
    "index = 200 ## randomly pick index\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(f'Human Baseline summary: \\n{human_baseline_summary}\\n')\n",
    "print(f'Original Model Output \\n{original_model_text_output}\\n')\n",
    "print(f'Peft Model Output \\n{peft_model_text_output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3834c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for _, dialogue in enumerate(dialogue):\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following conversations. \n",
    "\n",
    "    {dialogue}\n",
    "\n",
    "    Summary: \"\"\"\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries,\n",
    "                           peft_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns=['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3039728",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(predictions=original_model_summaries, \n",
    "                                       references=human_baseline_summaries[0: len(original_model_summaries)],\n",
    "                                      use_aggregator=True,\n",
    "                                      use_stemmer=True)\n",
    "\n",
    "peft_model_results = rouge.compute(predictions=peft_model_summaries, \n",
    "                                    references=human_baseline_summaries[0: len(peft_model_summaries)],\n",
    "                                    use_aggregator=True,\n",
    "                                    use_stemmer=True)\n",
    "\n",
    "print(f'Original Model: \\n{original_model_results}\\n') \n",
    "print(f'PEFT Model: \\n{peft_model_results}\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
